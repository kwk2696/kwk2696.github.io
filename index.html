<!DOCTYPE html>
<html lang="en">
    <head>
      <title> Woo Kyung Kim </title>
      <meta name="author" content="Woo Kyung Kim">
      <meta name="description" content="Woo Kyung Kim - PhD student at SungKyunKwan University. Research on skill-based reinforcement learning, diffusion models, and embodied agents.">
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <link rel="icon" type="image/svg+xml" href="favicon.svg">
      <!-- Open Graph -->
      <meta property="og:title" content="Woo Kyung Kim">
      <meta property="og:description" content="PhD student at SungKyunKwan University. Research on skill-based reinforcement learning, diffusion models, and embodied agents.">
      <meta property="og:image" content="https://kwk2696.github.io/images/WooKyungKim.jpg">
      <meta property="og:url" content="https://kwk2696.github.io">
      <meta property="og:type" content="website">
      <link rel="stylesheet" type="text/css" href="style.css">
      <script>
        (function() {
          var theme = localStorage.getItem('theme');
          if (!theme) {
            theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
          }
          if (theme === 'dark') {
            document.documentElement.setAttribute('data-theme', 'dark');
          }
        })();
      </script>
    </head>

  <body>
    <nav class="top-nav">
      <a href="index.html" class="active">Home</a>
      <a href="index.html#publications">Publications</a>
      <a href="blog.html">Blog</a>
      <button id="theme-toggle" aria-label="Toggle dark mode"></button>
    </nav>
    <section class="profile-section">
      <div class="profile-photo">
        <img alt="profile photo" src="images/WooKyungKim.jpg">
      </div>
      <div class="profile-info">
        <name>Woo Kyung Kim</name>
        <p>I am a PhD student at the <a href="https://sites.google.com/view/csi-agent-group/about?authuser=0">Computer Systems Intelligence (CSI) Lab</a> in SungKyunKwan University, advised by <a href="https://scholar.google.com/citations?user=Gaxjc7UAAAAJ&hl=ko">Prof. Honguk Woo</a>. I received my M.S. and B.S. in Computer Science and Engineering from the same university. My research focuses on <strong>skill-based reinforcement learning</strong>, <strong>diffusion models</strong>, and <strong>embodied agents</strong>.</p>
        <div class="profile-links">
          <a href="https://scholar.google.com/citations?user=OFFacb0AAAAJ">
            <svg class="link-icon" viewBox="0 0 24 24"><path d="M5.242 13.769L0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977 0-5.548 1.748-6.758 4.269zM12 10a7 7 0 1 0 0 14 7 7 0 0 0 0-14z"/></svg>
            Scholar
          </a>
          <a href="data/cv.pdf">
            <svg class="link-icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8l-6-6zm-1 1.5L18.5 9H13V3.5zM8 13h8v2H8v-2zm0 4h8v2H8v-2zm0-8h3v2H8V9z"/></svg>
            CV
          </a>
          <a href="https://github.com/kwk2696">
            <svg class="link-icon" viewBox="0 0 24 24"><path d="M12 0C5.374 0 0 5.373 0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23A11.509 11.509 0 0 1 12 5.803c1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
            Github
          </a>
          <a href="https://www.linkedin.com/in/woo-kyung-kim-3474242a2/">
            <svg class="link-icon" viewBox="0 0 24 24"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 0 1-2.063-2.065 2.064 2.064 0 1 1 2.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
            LinkedIn
          </a>
        </div>
      </div>
    </section>

    <div class="main-content">
<!--
      <hr class="section-divider">
      <h2 class="section-heading">News</h2>
      <ul class="news-list">
        <li><span class="news-date">2025.02</span> <span>Our paper "<a href="">In-Context Policy Adaptation via Cross-Domain Skill Diffusion</a>" has been accepted at <strong>AAAI 2025</strong>. We explore diffusion-based skill learning techniques for cross-domain in-context policy adaptation in long-horizon multi-task environments.</span></li>
        <li><span class="news-date">2024.09</span> <span>Two papers accepted at <strong>NeurIPS 2024</strong>: "<a href="https://openreview.net/pdf?id=UGlDVc0GTU">LLM-based Skill Diffusion for Zero-shot Policy Adaptation</a>" on leveraging LLMs to guide skill diffusion for zero-shot adaptation, and "<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/1f0832859514e53a0e4f229fc9b3a4a2-Paper-Conference.pdf">Incremental Learning of Retrievable Skills for Efficient Continual Task Adaptation</a>" on continual skill learning from demonstrations.</span></li>
        <li><span class="news-date">2024.05</span> <span>Our paper "<a href="https://openreview.net/pdf?id=M4Htd52HMH">Embodied CoT Distillation From LLM To Off-the-shelf Agents</a>" has been accepted at <strong>ICML 2024</strong>. We propose DEDER, a framework for distilling embodied chain-of-thought reasoning from LLMs to smaller policy models.</span></li>
        <li><span class="news-date">2024.04</span> <span>Our paper "<a href="https://arxiv.org/pdf/2408.12110">Pareto Inverse Reinforcement Learning for Diverse Expert Policy Generation</a>" has been accepted at <strong>IJCAI 2024</strong>. We present ParIRL for inducing Pareto-optimal policy sets over multi-objective tasks.</span></li>
      </ul>
-->

      <hr class="section-divider">
      <h2 class="section-heading">Education</h2>
      <ul class="education-list">
        <li>
          <strong>Ph.D.</strong> in Computer Science and Engineering, SungKyunKwan University
          <span class="edu-period">Sep. 2022 - Present</span>
          <div class="edu-detail">Specialist Research Personnel (전문연구요원), Sep. 2024 - Present</div>
        </li>
        <li>
          <strong>M.S.</strong> in Computer Science and Engineering, SungKyunKwan University
          <span class="edu-period">Mar. 2021 - Aug. 2022</span>
        </li>
        <li>
          <strong>B.S.</strong> in Computer Science and Engineering, SungKyunKwan University
          <span class="edu-period">Mar. 2017 - Feb. 2021</span>
        </li>
        <li>
          <strong>High School</strong>, Korean Minjok Leadership Academy (민족사관고등학교)
          <span class="edu-period">Mar. 2014 - Feb. 2017</span>
        </li>
      </ul>

      <hr class="section-divider">
      <h2 class="section-heading" id="publications">Conference Publications</h2>
      <div class="pub-section">

            <div class="pub-card">
              <div class="pub-card-img"><img src="images/ICPAD_AAAI25.png" alt="ICPAD"></div>
              <div class="pub-card-content">
                <a href=""><papertitle>In-Context Policy Adaptation via Cross-Domain Skill Diffusion</papertitle></a>
                <div class="authors"><a href="https://scholar.google.com/citations?user=O6L-PkgAAAAJ&hl=ko">Minjong Yoo</a>*, <strong>Woo Kyung Kim</strong>, <a href="https://scholar.google.com/citations?user=Gaxjc7UAAAAJ&hl=ko">Honguk Woo</a></div>
                <div class="venue"><em>AAAI</em>, 2025.02, Philadelphia, United States</div>
                <p class="description">In this work, we present an in-context policy adaptation (ICPAD) framework designed for long-horizon multi-task environments, exploring diffusion-based skill learning techniques in cross-domain settings.</p>              </div>
            </div>

            <div class="pub-card">
              <div class="pub-card-img"><img src="images/LDuS_NeurIPS24.png" alt="LDuS"></div>
              <div class="pub-card-content">
                <a href="https://openreview.net/pdf?id=UGlDVc0GTU"><papertitle>LLM-based Skill Diffusion for Zero-shot Policy Adaptation</papertitle></a>
                <div class="authors"><strong>Woo Kyung Kim</strong>*, <a href="https://scholar.google.com/citations?user=GhwtxtgAAAAJ">Youngseok Lee</a>, Jooyoung Kim, <a href="https://scholar.google.com/citations?user=Gaxjc7UAAAAJ&hl=ko">Honguk Woo</a></div>
                <div class="venue"><em>NeurIPS</em>, 2024.12, Vancouver, Canada</div>
                <p class="description">In this paper, we present a novel LLM-based policy adaptation framework LDuS which leverages an LLM to guide the generation process of a skill diffusion model upon contexts specified in language, facilitating zero-shot skill-based policy adaptation to different contexts.</p>
              </div>
            </div>

            <div class="pub-card">
              <div class="pub-card-img"><img src="images/IsCiL_NeurIPS24.png" alt="IsCiL"></div>
              <div class="pub-card-content">
                <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/1f0832859514e53a0e4f229fc9b3a4a2-Paper-Conference.pdf"><papertitle>Incremental Learning of Retrievable Skills for Efficient Continual Task Adaptation</papertitle></a>
                <div class="authors"><a href="https://scholar.google.com/citations?user=llB3SucAAAAJ">Daehee Lee</a>*, <a href="https://scholar.google.com/citations?user=O6L-PkgAAAAJ&hl=ko">Minjong Yoo</a>, <strong>Woo Kyung Kim</strong>, <a href="https://scholar.google.com/citations?user=L4d1CjEAAAAJ">Wonje Choi</a>, <a href="https://scholar.google.com/citations?user=Gaxjc7UAAAAJ&hl=ko">Honguk Woo</a></div>
                <div class="venue"><em>NeurIPS</em>, 2024.12, Vancouver, Canada</div>
                <p class="description">We introduce IsCiL, an adapter-based continual imitation learning framework that incrementally learns sharable skills from different demonstrations, enabling sample efficient task adaptation using the skills.</p>
              </div>
            </div>

            <div class="pub-card">
              <div class="pub-card-img"><img src="images/ParIRL_IJCAI24.png" alt="ParIRL"></div>
              <div class="pub-card-content">
                <a href="https://arxiv.org/pdf/2408.12110"><papertitle>Pareto Inverse Reinforcement Learning for Diverse Expert Policy Generation</papertitle></a>
                <div class="authors"><strong>Woo Kyung Kim</strong>*, <a href="https://scholar.google.com/citations?user=O6L-PkgAAAAJ&hl=ko">Minjong Yoo</a>, <a href="https://scholar.google.com/citations?user=Gaxjc7UAAAAJ&hl=ko">Honguk Woo</a></div>
                <div class="venue"><em>IJCAI</em>, 2024.08, Jeju, Korea</div>
                <p class="description">In this paper, we present Pareto inverse reinforcement learning (ParIRL) framework in which a Pareto policy set corresponding to the best compromise solutions over multi-objectives can be induced.</p>
              </div>
            </div>

            <div class="pub-card">
              <div class="pub-card-img"><img src="images/DEDER_ICML24.png" alt="DEDER"></div>
              <div class="pub-card-content">
                <a href="https://openreview.net/pdf?id=M4Htd52HMH"><papertitle>Embodied CoT Distillation From LLM To Off-the-shelf Agents</papertitle></a>
                <div class="authors"><a href="https://scholar.google.com/citations?user=L4d1CjEAAAAJ">Wonje Choi</a>*, <strong>Woo Kyung Kim</strong>, <a href="https://scholar.google.com/citations?user=O6L-PkgAAAAJ">Minjong Yoo</a>, <a href="https://scholar.google.com/citations?user=Gaxjc7UAAAAJ">Honguk Woo</a></div>
                <div class="venue"><em>ICML</em>, 2024.07, Wien, Austria</div>
                <p class="description">We present DEDER, a framework for decomposing and distilling the embodied reasoning capabilities from large language models (LLMs) to efficient, small language model (sLM)-based policies.</p>
              </div>
            </div>

            <div class="pub-card">
              <div class="pub-card-img"><img src="images/DuSkill_AAAI24.png" alt="DuSkill"></div>
              <div class="pub-card-content">
                <a href="https://arxiv.org/pdf/2403.00225"><papertitle>Robust Policy Learning via Offline Skill Diffusion</papertitle></a>
                <div class="authors"><strong>Woo Kyung Kim</strong>*, <a href="https://scholar.google.com/citations?user=O6L-PkgAAAAJ">Minjong Yoo</a>, <a href="https://scholar.google.com/citations?user=Gaxjc7UAAAAJ">Honguk Woo</a></div>
                <div class="venue"><em>AAAI</em>, 2024.02, Vancouver, Canada</div>
                <p class="description">We present a novel offline skill learning (DuSkill) framework which employs a guided Diffusion model to generate versatile skills extended from the limited skills in datasets, thereby enhancing the robustness of policy learning for tasks in different domains.</p>
              </div>
            </div>

            <div class="pub-card">
              <div class="pub-card-img"><img src="images/ConPE_NeurIPS23.png" alt="ConPE"></div>
              <div class="pub-card-content">
                <a href="https://openreview.net/pdf?id=Ny3GcHLyzj"><papertitle>Efficient Policy Adaptation with Contrastive Prompt Ensemble for Embodied Agents</papertitle></a>
                <div class="authors"><a href="https://scholar.google.com/citations?user=L4d1CjEAAAAJ">Wonje Choi</a>*, <strong>Woo Kyung Kim</strong>, SeungHyun Kim, <a href="https://scholar.google.com/citations?user=Gaxjc7UAAAAJ">Honguk Woo</a></div>
                <div class="venue"><em>NeurIPS</em>, 2023.12, New Orleans, United States</div>
                <p class="description">We present a novel contrastive prompt ensemble (ConPE) framework which utilizes a pretrained vision-language model and a set of visual prompts, thus enables efficient policy learning and adaptation upon environmental and physical changes encountered by embodied agents.</p>
              </div>
            </div>

            <div class="pub-card">
              <div class="pub-card-img"><img src="images/OnIS_ICML23.png" alt="OnIS"></div>
              <div class="pub-card-content">
                <a href="https://proceedings.mlr.press/v202/shin23d/shin23d.pdf"><papertitle>One-shot Imitation in a Non-stationary Environment via Multi-modal Skill</papertitle></a>
                <div class="authors"><a href="https://jsw7460.github.io/">Sangwoo Shin</a>*, <a href="https://scholar.google.com/citations?user=llB3SucAAAAJ">Daehee Lee</a>, <a href="https://scholar.google.com/citations?user=O6L-PkgAAAAJ">Minjong Yoo</a>, <strong>Woo Kyung Kim</strong>, <a href="https://scholar.google.com/citations?user=Gaxjc7UAAAAJ">Honguk Woo</a></div>
                <div class="venue"><em>ICML</em>, 2023.07, Honolulu, United States</div>
                <p class="description">In this paper, we explore the compositionality of complex tasks, and present a novel skill-based imitation learning (OnIS) framework enabling one-shot imitation and zero-shot adaptation.</p>
              </div>
            </div>

          </div>

      <hr class="section-divider">
      <h2 class="section-heading">Journal Publications</h2>
          <div class="pub-section">

            <div class="pub-card">
              <div class="pub-card-img"><img src="images/A2D2_ESWA.png" alt="A2D2"></div>
              <div class="pub-card-content">
                <a href="https://www.sciencedirect.com/science/article/pii/S0957417425041090"><papertitle>Aspect-Augmented Distillation of Task-Oriented Dialogues to Small Language Models</papertitle></a>
                <div class="authors">Jongmoon Jun, <strong>Woo Kyung Kim</strong>, Hyunseong Na, <a href="https://scholar.google.com/citations?user=Gaxjc7UAAAAJ">Honguk Woo</a>, Jeehyeong Kim</div>
                <div class="venue"><em>Expert Systems with Applications</em>, 2025.03. Volume 302, Page 130494</div>
                <p class="description">We present A2D2, an aspect-augmented dialogue distillation framework designed to transfer capabilities from larger language models to smaller ones for task-oriented dialogue systems, incorporating human aspect-aware capabilities while maintaining task requirements.</p>
              </div>
            </div>

            <div class="pub-card">
              <div class="pub-card-img"><img src="images/Repot_IEEE.png" alt="Repot"></div>
              <div class="pub-card-content">
                <a href="https://ieeexplore.ieee.org/abstract/document/9599665/"><papertitle>Repot: Transferable Reinforcement Learning for Quality-Centric Networked Monitoring in Various Environments</papertitle></a>
                <div class="authors"><a href="https://scholar.google.com/citations?user=GhwtxtgAAAAJ">Youngseok Lee</a>*, <strong>Woo Kyung Kim</strong>, Sung Hyun Choi, <a href="https://scholar.google.com/citations?user=Gaxjc7UAAAAJ">Honguk Woo</a></div>
                <div class="venue"><em>IEEE Access</em>, 2023.11. Volume 9, Page 147280-147294</div>
                <p class="description">In this paper, we present a transferable RL model Repot in which a policy trained in an easy-to-learn network environment can be readily adjusted in various target network environments.</p>
              </div>
            </div>

          </div>
    </div>
    <footer class="site-footer">
      &copy; 2026 Woo Kyung Kim. All rights reserved.
      <br>
      Total visitors: <img src="https://kwk2696.goatcounter.com/counter/.svg" alt="visitor count" style="vertical-align:middle;border:none;">
    </footer>
    <script data-goatcounter="https://kwk2696.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
    <script>
      (function() {
        var toggle = document.getElementById('theme-toggle');
        var isDark = document.documentElement.getAttribute('data-theme') === 'dark';
        toggle.textContent = isDark ? '\u2600\uFE0F' : '\uD83C\uDF19';

        toggle.addEventListener('click', function() {
          isDark = !isDark;
          if (isDark) {
            document.documentElement.setAttribute('data-theme', 'dark');
            localStorage.setItem('theme', 'dark');
            toggle.textContent = '\u2600\uFE0F';
          } else {
            document.documentElement.removeAttribute('data-theme');
            localStorage.setItem('theme', 'light');
            toggle.textContent = '\uD83C\uDF19';
          }
        });
      })();

      // Scroll-based nav highlight
      (function() {
        var sections = document.querySelectorAll('[id]');
        var navLinks = document.querySelectorAll('.top-nav a[href^="index.html"]');
        window.addEventListener('scroll', function() {
          var scrollPos = window.scrollY + 80;
          var pubSection = document.getElementById('publications');
          if (pubSection && scrollPos >= pubSection.offsetTop) {
            navLinks.forEach(function(link) { link.classList.remove('active'); });
            var pubLink = document.querySelector('.top-nav a[href="index.html#publications"]');
            if (pubLink) pubLink.classList.add('active');
          } else {
            navLinks.forEach(function(link) { link.classList.remove('active'); });
            var homeLink = document.querySelector('.top-nav a[href="index.html"]');
            if (homeLink) homeLink.classList.add('active');
          }
        });
      })();
    </script>
  </body>
</html>
