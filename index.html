<!DOCTYPE html>
<html lang="en">
    <head>
      <title> Woo Kyung Kim </title>
      <meta name="author" content="Woo Kyung Kim">
      <meta name="viewpoint" content="width=device-width, initial-scale=1">
      <link rel="stylesheet" type="text/css" href="style.css">
    </head>

  <body>
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center">
                  <name>Woo Kyung Kim</name>
                </p>
  
                <p>I am currently a PhD student at <a href="https://sites.google.com/view/csi-agent-group/about?authuser=0"> Computer Systems Intelligence Lab </a> in SungKyunKwan University, advised <a href="https://scholar.google.com/citations?user=Gaxjc7UAAAAJ&hl=ko">Honguk Woo</a>. My research areas include data-driven reinforcement learning, embodied agent, and foundation models.
                </p>
                <p style="text-align:center">
		    <a href="https://scholar.google.com/citations?user=OFFacb0AAAAJ">Google Scholar</a> &nbsp/&nbsp
                    <a href="data/cv.pdf">CV</a> &nbsp/&nbsp
		    <a href="https://github.com/kwk2696">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
		<a href="images/WooKyungKim.jpg"><img style="width:60%;max-width:60%" alt="profile photo" src="images/WooKyungKim.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
  	  <hr style="width:100%;">
          <h2>News</h2>
          <ul>
          </ul>
          <br>
	  <hr style="width:100%;">
          <h2>Conference Publications</h2>
	    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		<tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="images/ParIRL_IJCAI24.png" alt="ParIRL" style="border-style: none" width="300">
		</td>
                <td style="padding:20px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/abs/2408.12110"
                <papertitle>Pareto Inverse Reinforcement Learning for Diverse Expert Policy Generation</papertitle>
		</a>
                <br>
                <strong>Woo Kyung Kim</strong>*,
                <a href="https://scholar.google.com/citations?user=O6L-PkgAAAAJ&hl=ko">Minjong Yoo</a>,
                <a href="https://scholar.google.com/citations?user=Gaxjc7UAAAAJ&hl=ko">Honguk Woo</a>
                <br>
                <em>IJCAI</em>, 2024.08, Jeju, Korea
                <br>
                <p></p>
               	<p>In this paper, we present Pareto inverse reinforcement learning (ParIRL) framework in which a Pareto policy set corresponding to the best compromise solutions over multi-objectives can be induced.</p>
                </td>
                </tr>
	   </table>
	   <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		<tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="images/DEDER_ICML24.png" alt="DEDER" style="border-style: none" width="300">
		</td>
                <td style="padding:20px;width:75%;vertical-align:middle">
		<a href="https://openreview.net/forum?id=M4Htd52HMH"
                <papertitle>Embodied CoT Distillation From LLM To Off-the-shelf Agents</papertitle>
		</a>
                <br>
		<a href="https://scholar.google.com/citations?user=L4d1CjEAAAAJ">Wonje Choi</a>*,
                <strong>Woo Kyung Kim</strong>,
                <a href="https://scholar.google.com/citations?user=O6L-PkgAAAAJ">Minjong Yoo</a>,
                <a href="https://scholar.google.com/citations?user=Gaxjc7UAAAAJ">Honguk Woo</a>
                <br>
                <em>ICML</em>, 2024.07, Wien, Austria
                <br>
                <p></p>
               	<p>We present DEDER, a framework for decomposing and distilling the embodied reasoning capabilities from large language models (LLMs) to efficient, small language model (sLM)-based policies.</p>
                </td>
                </tr>
	   </table>
	   <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		<tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="images/DuSkill_AAAI24.png" alt="DuSkill" style="border-style: none" width="300">
		</td>
                <td style="padding:20px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/abs/2403.00225"
                <papertitle>Robust Policy Learning via Offline Skill Diffusion</papertitle>
		</a>
                <br>
                <strong>Woo Kyung Kim</strong>*,
                <a href="https://scholar.google.com/citations?user=O6L-PkgAAAAJ">Minjong Yoo</a>,
                <a href="https://scholar.google.com/citations?user=Gaxjc7UAAAAJ">Honguk Woo</a>
                <br>
                <em>AAAI</em>, 2024.02, Vancouver, Canada
                <br>
                <p></p>
               	<p>We present a novel offline skill learning (DuSkill) framework which employs a guided Diffusion model to generate versatile skills extended from the limited skills in datasets, thereby enhancing the robustness of policy learning for tasks in different domains.</p>
                </td>
                </tr>
	   </table>
	   <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		<tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="images/ConPE_NeurIPS23.png" alt="ConPE" style="border-style: none" width="300">
		</td>
                <td style="padding:20px;width:75%;vertical-align:middle">
		<a href="https://openreview.net/forum?id=Ny3GcHLyzj"
                <papertitle>Efficient Policy Adaptation with Contrastive Prompt Ensemble for Embodied Agents</papertitle>
		</a>
                <br>
		<a href="https://scholar.google.com/citations?user=L4d1CjEAAAAJ">Wonje Choi</a>*,
                <strong>Woo Kyung Kim</strong>,
		SeungHyun Kim,
                <a href="https://scholar.google.com/citations?user=Gaxjc7UAAAAJ">Honguk Woo</a>
                <br>
                <em>NeurIPS</em>, 2023.12, New Orleans, United States
                <br>
                <p></p>
               	<p>Wwe present a novel contrastive prompt ensemble (ConPE) framework which utilizes a pretrained vision-language model and a set of visual prompts, thus enables efficient policy learning and adaptation upon environmental and physical changes encountered by embodied agents.</p>
                </td>
                </tr>
	   </table>
	   <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		<tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="images/OnIS_ICML23.png" alt="OnIS" style="border-style: none" width="300">
		</td>
                <td style="padding:20px;width:75%;vertical-align:middle">
		<a href="https://proceedings.mlr.press/v202/shin23d.html"
                <papertitle>Efficient Policy Adaptation with Contrastive Prompt Ensemble for Embodied Agents</papertitle>
		</a>
                <br>
		<a href="https://jsw7460.github.io/">Sangwoo Shin</a>*,
		<a href="https://scholar.google.com/citations?user=llB3SucAAAAJ">Daehee Lee</a>,
		<a href="https://scholar.google.com/citations?user=O6L-PkgAAAAJ">Minjong Yoo</a>,
                <strong>Woo Kyung Kim</strong>,
                <a href="https://scholar.google.com/citations?user=Gaxjc7UAAAAJ">Honguk Woo</a>
                <br>
                <em>ICML</em>, 2023.07, Honolulu, United States
                <br>
                <p></p>
               	<p>In this paper, we explore the compositionality of complex tasks, and present a novel skill-based imitation learning (OnIS) framework enabling one-shot imitation and zero-shot adaptation.</p>
                </td>
                </tr>
	   </table>
	<hr style="width:100%;">
        <h2>Journal Publications</h2>
	   <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		<tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="images/Repot_IEEE.png" alt="Repot" style="border-style: none" width="300">
		</td>
                <td style="padding:20px;width:75%;vertical-align:middle">
		<a href="https://ieeexplore.ieee.org/abstract/document/9599665/"
                <papertitle>Repot: Transferable Reinforcement Learning for Quality-Centric Networked Monitoring in Various Environments</papertitle>
		</a>
                <br>
		<a href="https://scholar.google.com/citations?user=GhwtxtgAAAAJ">Youngseok Lee</a>*,
                <strong>Woo Kyung Kim</strong>,
		Sung Hyun Choi,
                <a href="https://scholar.google.com/citations?user=Gaxjc7UAAAAJ">Honguk Woo</a>
                <br>
                <em>IEE Access</em>, 2023.11. Volume 9, Page 147280-147294
                <br>
                <p></p>
               	<p> In this paper, we present a transferable RL model Repot in which a policy trained in an easy-to-learn network environment can be readily adjusted in various target network environments.</p>
                </td>
                </tr>
	   </table>
    	</td>
      </tr>
    </table>
  </body>
</html>
